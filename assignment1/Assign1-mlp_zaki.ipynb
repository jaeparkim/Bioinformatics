{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cc4770f-7aac-4d16-9247-35b01ed82d27",
   "metadata": {},
   "source": [
    "import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6b0ef31-0dc4-4280-84e0-019652c7a2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib as jl\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as du\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e1cbba-7bb3-4a80-bc9c-5ee4c94b45e9",
   "metadata": {},
   "source": [
    "read JUND data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb14f710-b039-4436-b523-24dbaa8acc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JUND_Dataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        '''load X, y, w, a from data_dir\n",
    "        convert all to float tensors'''\n",
    "        super(JUND_Dataset, self).__init__()\n",
    "        cur_dir = os.path.join(os.getcwd(), data_dir)\n",
    "        self.X = jl.load(os.path.join(cur_dir, 'shard-0-X.joblib'))\n",
    "        self.X = torch.tensor(self.X, dtype=torch.float)\n",
    "        self.y = jl.load(os.path.join(cur_dir, 'shard-0-y.joblib'))\n",
    "        self.y = torch.tensor(self.y, dtype=torch.float)\n",
    "        self.w = jl.load(os.path.join(cur_dir, 'shard-0-w.joblib'))\n",
    "        self.w = torch.tensor(self.w, dtype=torch.float)\n",
    "        self.a = jl.load(os.path.join(cur_dir, 'shard-0-a.joblib'))\n",
    "        self.a = torch.tensor(self.a, dtype=torch.float)\n",
    "        \n",
    "    def __len__(self):\n",
    "        '''return len of dataset'''\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        '''return X, y, w, and a values at index idx'''\n",
    "        return self.X[idx,:], self.y[idx], self.w[idx], self.a[idx]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7cca164-b69e-420e-81f5-9f2121fd8fcd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train X torch.Size([276216, 101, 4])\n",
      "test X torch.Size([34528, 101, 4])\n",
      "val X torch.Size([34527, 101, 4])\n",
      "frac train pos 0.004228574738610363\n",
      "frac val pos 0.004228574738610363\n",
      "frac test pos 0.0042284522706209455\n",
      "w_sum tensor(276215.9375) tensor(34526.9961) tensor(34527.9961)\n"
     ]
    }
   ],
   "source": [
    "# create datasets and print basic stats\n",
    "train_data = JUND_Dataset('train_dataset')\n",
    "test_data = JUND_Dataset('test_dataset')\n",
    "val_data = JUND_Dataset('valid_dataset')\n",
    "print(\"train X\", train_data.X.shape)\n",
    "print(\"test X\", test_data.X.shape)\n",
    "print(\"val X\", val_data.X.shape)\n",
    "print(\"frac train pos\", len(torch.where(train_data.y == 1)[0])/train_data.y.shape[0])\n",
    "print(\"frac val pos\", len(torch.where(val_data.y == 1)[0])/val_data.y.shape[0])\n",
    "print(\"frac test pos\", len(torch.where(test_data.y == 1)[0])/test_data.y.shape[0])\n",
    "print(\"w_sum\", train_data.w.sum(), val_data.w.sum(), test_data.w.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c3036a-d4ac-42f5-bb56-3f8f2a3e6727",
   "metadata": {},
   "source": [
    "define MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20ddd8c2-cf48-45c4-8656-9609c04a519e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, use_a=True,\n",
    "                 dropout=0):\n",
    "        '''in_dim: input layer dim\n",
    "           hidden_dim: hidden layer dim\n",
    "           out_dim: output layer dim\n",
    "           use_a: use accessibility value?\n",
    "           dropout: dropout probability\n",
    "           '''\n",
    "        \n",
    "        super(MLP, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.use_a = use_a\n",
    "            \n",
    "        # input is 101 x 4 array, so flatten into 404d vec\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        #two fully connected layers\n",
    "        self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
    "        \n",
    "        #concat a or not? accessibility info\n",
    "        if use_a:\n",
    "            hidden_dim += 1\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        #x is Bx101x4, a is Bx1 accessibility values          \n",
    "        # since x is 101x4, flatten it first\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        # compute output of fc1, and apply relu activation, followed by dropout\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, p=self.dropout)\n",
    "        \n",
    "        # concat x and a\n",
    "        if self.use_a:\n",
    "            x = torch.cat([x, a], dim=1)\n",
    "            \n",
    "       # compute output layer\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e002c1c0-3f12-485e-88ab-4636af034fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_correct(output, target, weight):\n",
    "    '''compute the weights for correct prediction\n",
    "       first apply sigmoid and predict class 1 if >= 0.5, 0 otherwise\n",
    "    '''\n",
    "    #use logsigmoid for log space computations\n",
    "    output = F.logsigmoid(output.detach())\n",
    "    pred = torch.where(output > F.logsigmoid(torch.tensor(0.5)), \n",
    "                       1, 0)\n",
    "\n",
    "    # add up weights of correct predictions\n",
    "    correct = torch.sum((pred == target)*weight)\n",
    "    \n",
    "    return correct.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9cd916-d5a4-4081-b591-510221005173",
   "metadata": {},
   "source": [
    "Evaluation Loop: Used for Validation and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ed92ca7-7f35-40c7-a74a-7234603e6dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader):\n",
    "    # set model in eval mode, since we are no longer training\n",
    "    model.eval()\n",
    "    eval_loss = 0.\n",
    "    correct = 0.\n",
    "    \n",
    "    # turn of gradient computation, will speed up testing\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target, weight, accessibility)\\\n",
    "            in enumerate(data_loader):\n",
    "            # send batches to device\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            weight = weight.to(device)\n",
    "            accessibility = accessibility.to(device)\n",
    "\n",
    "            # compute forward pass and loss\n",
    "            output = model(data, accessibility)\n",
    "            loss = F.binary_cross_entropy_with_logits(\n",
    "                output, target, weight=weight)\n",
    "\n",
    "            # sum up batch loss\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            # add up number of correct predictions\n",
    "            correct += compute_correct(output, target, weight)\n",
    "            \n",
    "        # eval loss per example\n",
    "        eval_loss /= (batch_idx+1)\n",
    "\n",
    "        # final test accuracy\n",
    "        eval_acc = correct / data_loader.dataset.w.sum().item()\n",
    "\n",
    "    #put model back to training mode at end of eval\n",
    "    model.train()\n",
    "    return eval_loss, eval_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19109927-87a6-45fb-85c5-d5bd16e0e847",
   "metadata": {},
   "source": [
    "Set up training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fbd31a8-6919-4e9b-acc9-82575f284b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = f'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"using device: {device}\")\n",
    "batch_size = 1000\n",
    "learning_rate = 0.5\n",
    "epochs = 5\n",
    "weight_decay = 0.\n",
    "dropout = 0.5\n",
    "use_a = True\n",
    "\n",
    "# set model and optimizer\n",
    "# input is 101x4 array \n",
    "# use 128d hidden layer\n",
    "# output is 1d since there are 2 classes; use sigmoid\n",
    "\n",
    "model = MLP(101*4, 128, 1, use_a, dropout)\n",
    "    \n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate,\n",
    "                      weight_decay=weight_decay)\n",
    "\n",
    "# load training and validation data in batches\n",
    "train_loader = du.DataLoader(dataset=train_data,\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=True)\n",
    "\n",
    "val_loader = du.DataLoader(dataset=val_data,\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=True)\n",
    "# send model over to device\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fb1cbe-2d79-4554-9a69-edad36523645",
   "metadata": {},
   "source": [
    "Training loop over batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1c2933d-a886-4639-8a1c-c331b99af143",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 1.315484e+01, Acc: 0.5552\n",
      "Epoch: 2, Loss: 6.733035e+00, Acc: 0.6379\n",
      "Epoch: 3, Loss: 6.945383e-01, Acc: 0.6705\n",
      "Epoch: 4, Loss: 8.903910e-01, Acc: 0.6819\n",
      "Epoch: 5, Loss: 5.680076e-01, Acc: 0.6955\n",
      "Val Loss: 5.334630e-01, Val Acc: 0.7201\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    sum_loss = 0.\n",
    "    correct = 0.\n",
    "    for batch_idx, (data, target, weight, accessibility)\\\n",
    "        in enumerate(train_loader):\n",
    "        # send batch over to device\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        weight = weight.to(device)\n",
    "        accessibility = accessibility.to(device)\n",
    "        \n",
    "        # zero out prev gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # run the forward pass\n",
    "        output = model(data, accessibility)\n",
    "        # compute loss/error with weight per sample\n",
    "        loss = F.binary_cross_entropy_with_logits(\n",
    "                output, target, weight=weight)\n",
    "        sum_loss += loss.item()\n",
    "        \n",
    "        #compute training accuracy\n",
    "        correct += compute_correct(output, target, weight)\n",
    "\n",
    "        # compute gradients and take a step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # average loss per example\n",
    "    sum_loss /= (batch_idx+1)\n",
    "    train_acc = correct / train_data.w.sum().item()\n",
    "    print(f'Epoch: {epoch}, Loss: {sum_loss:.6e}, Acc: {train_acc:.4f}')\n",
    "    if epoch % 5 == 0:  \n",
    "        #how does validation do?\n",
    "        val_loss, val_acc = eval_model(model, val_loader)\n",
    "        print(f'Val Loss: {val_loss:.6e}, Val Acc: {val_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec70126a-7645-4d04-834d-921f51274a15",
   "metadata": {},
   "source": [
    "Now do testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "427a73e2-12a3-4e82-9951-7fd9abbbcf9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 5.249908e-01, accuracy: 0.7198\n"
     ]
    }
   ],
   "source": [
    "# load test data in batches\n",
    "test_loader = du.DataLoader(dataset=test_data,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=False)\n",
    "test_loss, test_acc = eval_model(model, test_loader)\n",
    "print(f'Test loss: {test_loss:.6e}, accuracy: {test_acc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cc4770f-7aac-4d16-9247-35b01ed82d27",
   "metadata": {},
   "source": [
    "import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6b0ef31-0dc4-4280-84e0-019652c7a2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as functional\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as du\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import os\n",
    "import time\n",
    "from joblib import load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac281ba-ef3c-4af1-8779-ad801bba6506",
   "metadata": {},
   "source": [
    "read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ee82fd4-d312-4a77-a4f4-23e7eae23ad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntrain = os.listdir('train_dataset/')\\nvalid = os.listdir('valid_dataset/')\\ntest = os.listdir('test_dataset/')\\n\\ntrain_x = load('train_dataset/' + 'shard-0-X.joblib') # (276216, 101, 1)\\nvalid_x = load('valid_dataset/' + valid[6]) # (34527, 101, 4)\\ntest_x = load('test_dataset/' + test[6]) # (34528, 101, 1)\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "train = os.listdir('train_dataset/')\n",
    "valid = os.listdir('valid_dataset/')\n",
    "test = os.listdir('test_dataset/')\n",
    "\n",
    "train_x = load('train_dataset/' + 'shard-0-X.joblib') # (276216, 101, 1)\n",
    "valid_x = load('valid_dataset/' + valid[6]) # (34527, 101, 4)\n",
    "test_x = load('test_dataset/' + test[6]) # (34528, 101, 1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02af9fa2-096b-45e3-9913-90df8baa06c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JUND_Dataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        '''load X, y, w, a from data_dir'''        \n",
    "        super(JUND_Dataset, self).__init__()\n",
    "\n",
    "        # load X, y, w, a from given data_dir\n",
    "        # convert them into torch tensors\n",
    "\n",
    "        x = load(data_dir + 'shard-0-X.joblib')\n",
    "        y = load(data_dir +'shard-0-y.joblib')\n",
    "        w = load(data_dir + 'shard-0-w.joblib')\n",
    "        # ids = load(data_dir + 'shard-0-ids.joblib')\n",
    "        a = load(data_dir + 'shard-0-a.joblib')\n",
    "\n",
    "        self.x = torch.tensor(x).float()  \n",
    "        self.y = torch.tensor(y).float()\n",
    "        self.w = torch.tensor(w).float()\n",
    "        # self.ids = torch.tensor(ids)\n",
    "        self.a = torch.tensor(a).float()\n",
    "        \n",
    "    def __len__(self):\n",
    "        '''return len of dataset'''\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        '''return X, y, w, and a values at index idx'''\n",
    "       \n",
    "        _x = self.x[idx]\n",
    "        _y = self.y[idx]\n",
    "        _w = self.w[idx]\n",
    "        # _ids = self.ids[idx]\n",
    "        _a = self.a[idx]\n",
    "        \n",
    "        return _x, _y, _w, _a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c3036a-d4ac-42f5-bb56-3f8f2a3e6727",
   "metadata": {},
   "source": [
    "define MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20ddd8c2-cf48-45c4-8656-9609c04a519e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "        '''in_dim: input layer dim\n",
    "           hidden_dim: hidden layer dim\n",
    "           out_dim: output layer dim'''\n",
    "        \n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        # images are 28x28 so flatten them into 784d vec\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        #two fully connected layers\n",
    "        self.fc1 = nn.Linear(in_dim, hidden_dim-1)\n",
    "        self.fc2 = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        # since x is 28x28, flatten it first\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        # compute output of fc1, and apply relu activation\n",
    "        x = functional.relu(self.fc1(x))\n",
    "        \n",
    "        # compute output layer\n",
    "        # no activation\n",
    "        x = torch.cat((x, a), dim=1)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19109927-87a6-45fb-85c5-d5bd16e0e847",
   "metadata": {},
   "source": [
    "Set up training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fbd31a8-6919-4e9b-acc9-82575f284b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=404, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=129, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = f'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"using device: {device}\")\n",
    "batch_size = 1000\n",
    "learning_rate = 0.5 #0.01\n",
    "epochs = 30\n",
    "\n",
    "# set model and optimizer\n",
    "# 104*4 as inputs\n",
    "# use 128 hidden layer + 1 accessibility value node\n",
    "# output is binary so 1\n",
    "model = MLP(101*4, 129, 1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# load training data in batches\n",
    "train_loader = du.DataLoader(dataset=JUND_Dataset('train_dataset/'),\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=True)\n",
    "# send model over to device\n",
    "model = model.to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fb1cbe-2d79-4554-9a69-edad36523645",
   "metadata": {},
   "source": [
    "Training loop over batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1c2933d-a886-4639-8a1c-c331b99af143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.000536\n",
      "Epoch: 2, Loss: 0.000535\n",
      "Epoch: 3, Loss: 0.000538\n",
      "Epoch: 4, Loss: 0.000538\n",
      "Epoch: 5, Loss: 0.000536\n",
      "Epoch: 6, Loss: 0.000536\n",
      "Epoch: 7, Loss: 0.000532\n",
      "Epoch: 8, Loss: 0.000533\n",
      "Epoch: 9, Loss: 0.000538\n",
      "Epoch: 10, Loss: 0.000537\n",
      "Epoch: 11, Loss: 0.000535\n",
      "Epoch: 12, Loss: 0.000535\n",
      "Epoch: 13, Loss: 0.000538\n",
      "Epoch: 14, Loss: 0.000536\n",
      "Epoch: 15, Loss: 0.000533\n",
      "Epoch: 16, Loss: 0.000537\n",
      "Epoch: 17, Loss: 0.000541\n",
      "Epoch: 18, Loss: 0.000532\n",
      "Epoch: 19, Loss: 0.000533\n",
      "Epoch: 20, Loss: 0.000535\n",
      "Epoch: 21, Loss: 0.000534\n",
      "Epoch: 22, Loss: 0.000543\n",
      "Epoch: 23, Loss: 0.000535\n",
      "Epoch: 24, Loss: 0.000541\n",
      "Epoch: 25, Loss: 0.000532\n",
      "Epoch: 26, Loss: 0.000540\n",
      "Epoch: 27, Loss: 0.000536\n",
      "Epoch: 28, Loss: 0.000530\n",
      "Epoch: 29, Loss: 0.000541\n",
      "Epoch: 30, Loss: 0.000533\n",
      "time:  96.86818504333496\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "for epoch in range(1, epochs + 1):    \n",
    "    sum_loss = 0.\n",
    "    \n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        # send batch over to device\n",
    "        x, y, w, a = data\n",
    "        x, y, a, w = x.to(device), y.to(device), a.to(device), w.to(device)\n",
    "\n",
    "        # zero out prev gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # run the forward pass\n",
    "        output = model(x, a)\n",
    "        \n",
    "        # compute loss/error\n",
    "        loss = functional.binary_cross_entropy_with_logits(output, y, weight=w)\n",
    "   \n",
    "            \n",
    "        # sum up batch losses\n",
    "        sum_loss += loss.item()\n",
    "        \n",
    "        # compute gradients and take a step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # average loss per example    \n",
    "    sum_loss /= len(train_loader.dataset)\n",
    "    print(f'Epoch: {epoch}, Loss: {sum_loss:.6f}')\n",
    "\n",
    "end = time.time()\n",
    "print(\"time: \", end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec70126a-7645-4d04-834d-921f51274a15",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "427a73e2-12a3-4e82-9951-7fd9abbbcf9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.000528, accuracy: 0.9044 correct: 31227\n"
     ]
    }
   ],
   "source": [
    "# load test images in batches\n",
    "test_loader = du.DataLoader(dataset=JUND_Dataset('test_dataset/'),\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=True)\n",
    "\n",
    "# set model in eval mode, since we are no longer training\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "# turn of gradient computation, will speed up testing\n",
    "with torch.no_grad():\n",
    "    for batch_idx, data in enumerate(test_loader):\n",
    "        # send batches to device\n",
    "        x, y, w, a = data\n",
    "        x, y, a, w = x.to(device), y.to(device), a.to(device), w.to(device)\n",
    "        \n",
    "        # compute forward pass\n",
    "        output = model(x, a)\n",
    "\n",
    "        # compute loss/error\n",
    "        loss = functional.binary_cross_entropy_with_logits(output, y, weight=w)\n",
    "        \n",
    "        # sum up batch loss\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # get the index/class of the max log-probability\n",
    "        # output = F.log_softmax(output, dim=1)\n",
    "        # pred = output.max(dim=1)[1]\n",
    "\n",
    "        m = nn.Sigmoid()\n",
    "        output = m(output)\n",
    "        \n",
    "        output = torch.where(output<0.5, output, torch.tensor(1.).to(device))\n",
    "        output = torch.where(output==1.0, output, torch.tensor(0.).to(device))\n",
    "\n",
    "        # add up number of correct predictions\n",
    "        correct += torch.sum(output == y)\n",
    "  \n",
    "    # test loss per example\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    \n",
    "    # final test accuracy\n",
    "    test_acc = correct / len(test_loader.dataset)\n",
    "    print(f'Test loss: {test_loss:.6f}, accuracy: {test_acc:.4f}',\n",
    "          f'correct: {correct}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c4a5cd-970d-48fc-989a-e243fcb27773",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "71b0def66c9fdfac05edfafc9587f9dcfd34e64b1b79e62a2a9108c6900c0515"
  },
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

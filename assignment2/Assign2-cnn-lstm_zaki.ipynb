{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cc4770f-7aac-4d16-9247-35b01ed82d27",
   "metadata": {},
   "source": [
    "import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6b0ef31-0dc4-4280-84e0-019652c7a2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib as jl\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as du\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e1cbba-7bb3-4a80-bc9c-5ee4c94b45e9",
   "metadata": {},
   "source": [
    "read JUND data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb14f710-b039-4436-b523-24dbaa8acc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JUND_Dataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        '''load X, y, w, a from data_dir\n",
    "        convert all to float tensors'''\n",
    "        super(JUND_Dataset, self).__init__()\n",
    "        cur_dir = os.path.join(os.getcwd(), data_dir)\n",
    "        self.X = jl.load(os.path.join(cur_dir, 'shard-0-X.joblib'))\n",
    "        self.X = torch.tensor(self.X, dtype=torch.float)\n",
    "        self.y = jl.load(os.path.join(cur_dir, 'shard-0-y.joblib'))\n",
    "        self.y = torch.tensor(self.y, dtype=torch.float)\n",
    "        self.w = jl.load(os.path.join(cur_dir, 'shard-0-w.joblib'))\n",
    "        self.w = torch.tensor(self.w, dtype=torch.float)\n",
    "        self.a = jl.load(os.path.join(cur_dir, 'shard-0-a.joblib'))\n",
    "        self.a = torch.tensor(self.a, dtype=torch.float)\n",
    "        \n",
    "    def __len__(self):\n",
    "        '''return len of dataset'''\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        '''return X, y, w, and a values at index idx'''\n",
    "        return self.X[idx,:], self.y[idx], self.w[idx], self.a[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7cca164-b69e-420e-81f5-9f2121fd8fcd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train X torch.Size([276216, 101, 4])\n",
      "test X torch.Size([34528, 101, 4])\n",
      "val X torch.Size([34527, 101, 4])\n",
      "frac train pos 0.004228574738610363\n",
      "frac val pos 0.004228574738610363\n",
      "frac test pos 0.0042284522706209455\n",
      "w_sum tensor(276215.9375) tensor(34526.9961) tensor(34527.9961)\n"
     ]
    }
   ],
   "source": [
    "# create datasets and print basic stats\n",
    "train_data = JUND_Dataset('train_dataset')\n",
    "test_data = JUND_Dataset('test_dataset')\n",
    "val_data = JUND_Dataset('valid_dataset')\n",
    "print(\"train X\", train_data.X.shape)\n",
    "print(\"test X\", test_data.X.shape)\n",
    "print(\"val X\", val_data.X.shape)\n",
    "print(\"frac train pos\", len(torch.where(train_data.y == 1)[0])/train_data.y.shape[0])\n",
    "print(\"frac val pos\", len(torch.where(val_data.y == 1)[0])/val_data.y.shape[0])\n",
    "print(\"frac test pos\", len(torch.where(test_data.y == 1)[0])/test_data.y.shape[0])\n",
    "print(\"w_sum\", train_data.w.sum(), val_data.w.sum(), test_data.w.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef82514-fde3-432f-997a-e17ccfb7d2b5",
   "metadata": {},
   "source": [
    "define LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8353f581-92e3-4a0d-934b-ea48a1af0752",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim_lstm, hidden_dim_mlp, use_a=True, dropout=0):\n",
    "        '''input_dim: input layer dim (# features)\n",
    "           hidden_dim_lstm: hidden layer dim for LSTM\n",
    "           hidden_dim_mlp: hidden layer for MLP\n",
    "           use_a: use accessibility\n",
    "           dropout: dropout prob'''\n",
    "\n",
    "        super(LSTM, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.use_a = use_a\n",
    "        \n",
    "        #use batch first\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim_lstm, batch_first=True)\n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_dim_lstm, hidden_dim_mlp)\n",
    "        \n",
    "        if use_a:\n",
    "            hidden_dim_mlp += 1\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_dim_mlp, 1)\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        # x is Bx101x4, a is Bx1 accessibility values\n",
    "\n",
    "        #note the last hidden layer state from LSTM\n",
    "        _, (hn, _) = self.lstm(x)\n",
    "        \n",
    "        #remove the first dim to \"flatten\" the hn state vector\n",
    "        hn = torch.squeeze(hn, dim=0)\n",
    "        \n",
    "        x = self.fc1(hn)\n",
    "\n",
    "        #add accessibility\n",
    "        if self.use_a:\n",
    "            x = torch.cat([x, a], dim=1)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f4f1c3-09b4-4035-b50c-89de9fc913e8",
   "metadata": {},
   "source": [
    "util function to get final size from CNN (useful when using diff conv, maxpool layers, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4fa143db-db5b-4617-8502-b5a42a87ba15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sz(model_list, in_val):\n",
    "    #generate random data of size in_val\n",
    "    in_dat = torch.rand(*(in_val))\n",
    "    #run thru layers and get number of elements\n",
    "    for model in model_list:\n",
    "        in_dat = model(in_dat).data\n",
    "    # return number of elements\n",
    "    return torch.numel(in_dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c3036a-d4ac-42f5-bb56-3f8f2a3e6727",
   "metadata": {},
   "source": [
    "define CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20ddd8c2-cf48-45c4-8656-9609c04a519e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, seqlen, in_channels, out_dim, hid_channels, kernel_size, \n",
    "                 hidden_dim, use_a=True, dropout=0, use_maxpool = False):\n",
    "        '''seqlen: length of input seq\n",
    "           in_channels: dim for input seq\n",
    "           out_dim: output layer dim\n",
    "           hid_channels: hidden layer channels\n",
    "           kernel_size: CNN kernel size\n",
    "           hidden_dim: for hidden layer of MLP\n",
    "           use_a: append accessibilty\n",
    "           dropout: use dropout prob\n",
    "           use_maxpool: use max pooling?\n",
    "           '''\n",
    "        super(CNN, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.use_a = use_a\n",
    "        self.use_maxpool = use_maxpool\n",
    "        \n",
    "        layers = []\n",
    "        self.conv1 = nn.Conv1d(in_channels, hid_channels, kernel_size)\n",
    "        layers.append(self.conv1)\n",
    "    \n",
    "        if self.use_maxpool:\n",
    "            self.maxpool1 = nn.MaxPool1d(2) #use kernel_size 2 for maxpool\n",
    "            layers.append(self.maxpool1)\n",
    "            \n",
    "        self.flatten = nn.Flatten()\n",
    "       \n",
    "        #figure out final layer size     \n",
    "        input_sz = (1, in_channels, seqlen)\n",
    "        lin_dim = get_sz(layers, input_sz)\n",
    "        \n",
    "        self.fc1 = nn.Linear(lin_dim, hidden_dim)\n",
    "        \n",
    "        #add accessibility\n",
    "        if use_a:\n",
    "            hidden_dim += 1\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_dim, out_dim)\n",
    " \n",
    "    def forward(self, x, a):\n",
    "        #x is Bx4x101, a is Bx1 accessibility values\n",
    "        # first swap x axes 1 and 2, since CNN expects channels x seqlen\n",
    "        #make x  Bx101x4\n",
    "        x = x.swapaxes(1,2)\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.dropout(x, p=self.dropout)\n",
    "        \n",
    "        if self.use_maxpool:\n",
    "            x = self.maxpool1(x)\n",
    "        \n",
    "        #flatten the last conv layer\n",
    "        x = self.flatten(x)\n",
    "       \n",
    "        # compute hidden layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, p=self.dropout)\n",
    "        \n",
    "        # concat accessibility\n",
    "        if self.use_a:\n",
    "            x = torch.cat([x, a], dim=1)\n",
    "            \n",
    "        # compute output layer\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e002c1c0-3f12-485e-88ab-4636af034fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_correct(output, target, weight):\n",
    "    '''compute the weights for correct prediction\n",
    "       first apply sigmoid and predict class 1 if >= 0.5, 0 otherwise\n",
    "    '''\n",
    "    #use logsigmoid for log space computations\n",
    "    output = F.logsigmoid(output.detach())\n",
    "    pred = torch.where(output > F.logsigmoid(torch.tensor(0.5)), \n",
    "                       1, 0)\n",
    "\n",
    "    # add up weights of correct predictions\n",
    "    correct = torch.sum((pred == target)*weight)\n",
    "    \n",
    "    return correct.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b8b62a-3ac6-4bf1-9b63-513a9e1c13ec",
   "metadata": {},
   "source": [
    "Evaluation Loop: Used for Validation and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ed92ca7-7f35-40c7-a74a-7234603e6dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader):\n",
    "    # set model in eval mode, since we are no longer training\n",
    "    model.eval()\n",
    "    eval_loss = 0.\n",
    "    correct = 0.\n",
    "    \n",
    "    # turn of gradient computation, will speed up testing\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target, weight, accessibility)\\\n",
    "            in enumerate(data_loader):\n",
    "            # send batches to device\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            weight = weight.to(device)\n",
    "            accessibility = accessibility.to(device)\n",
    "\n",
    "            # compute forward pass and loss\n",
    "            output = model(data, accessibility)\n",
    "            loss = F.binary_cross_entropy_with_logits(\n",
    "                output, target, weight=weight)\n",
    "\n",
    "            # sum up batch loss\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            # add up number of correct predictions\n",
    "            correct += compute_correct(output, target, weight)\n",
    "            \n",
    "        # eval loss per example\n",
    "        eval_loss /= (batch_idx+1)\n",
    "\n",
    "        # final test accuracy\n",
    "        eval_acc = correct / data_loader.dataset.w.sum().item()\n",
    "\n",
    "    #put model back to training mode at end of eval\n",
    "    model.train()\n",
    "    return eval_loss, eval_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c225512-6866-4657-91e0-a045b88db6b0",
   "metadata": {},
   "source": [
    "Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "729a7320-2873-448a-96f0-09352aa8d4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model):\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        sum_loss = 0.\n",
    "        correct = 0.\n",
    "        for batch_idx, (data, target, weight, accessibility)\\\n",
    "            in enumerate(train_loader):\n",
    "            # send batch over to device\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            weight = weight.to(device)\n",
    "            accessibility = accessibility.to(device)\n",
    "\n",
    "            # zero out prev gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # run the forward pass\n",
    "            output = model(data, accessibility)\n",
    "            # compute loss/error with weight per sample\n",
    "            loss = F.binary_cross_entropy_with_logits(\n",
    "                    output, target, weight=weight)\n",
    "            sum_loss += loss.item()\n",
    "\n",
    "            #compute training accuracy\n",
    "            correct += compute_correct(output, target, weight)\n",
    "\n",
    "            # compute gradients and take a step\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # average loss per example\n",
    "        sum_loss /= (batch_idx+1)\n",
    "        train_acc = correct / train_data.w.sum().item()\n",
    "        print(f'Epoch: {epoch}, Loss: {sum_loss:.6e}, Acc: {train_acc:.4f}')\n",
    "        if epoch % 1 == 0:  \n",
    "            #how does validation do\n",
    "            val_loss, val_acc = eval_model(model, val_loader)\n",
    "            print(f'Val Loss: {val_loss:.6e}, Val Acc: {val_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19109927-87a6-45fb-85c5-d5bd16e0e847",
   "metadata": {},
   "source": [
    "Set up training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3fbd31a8-6919-4e9b-acc9-82575f284b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = f'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "# load data in batches\n",
    "train_loader = du.DataLoader(dataset=train_data,\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=True)\n",
    "\n",
    "val_loader = du.DataLoader(dataset=val_data,\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=False)\n",
    "\n",
    "test_loader = du.DataLoader(dataset=test_data,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05911af7-6590-4b7d-9305-d49039be13f8",
   "metadata": {},
   "source": [
    "LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6b3076a7-6449-4864-9d29-bbf0b201610a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 4.762763e+01, Acc: 0.5124\n",
      "Val Loss: 6.602876e-01, Val Acc: 0.5236\n",
      "Epoch: 2, Loss: 6.273109e-01, Acc: 0.6070\n",
      "Val Loss: 6.034506e-01, Val Acc: 0.6157\n",
      "Epoch: 3, Loss: 2.592166e+02, Acc: 0.5480\n",
      "Val Loss: 5.853099e+00, Val Acc: 0.5000\n",
      "Epoch: 4, Loss: 2.113618e+01, Acc: 0.5876\n",
      "Val Loss: 6.763994e-01, Val Acc: 0.6458\n",
      "Epoch: 5, Loss: 6.384380e-01, Acc: 0.6796\n",
      "Val Loss: 5.853848e-01, Val Acc: 0.7379\n",
      "Test loss: 5.757445e-01, accuracy: 0.7316\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "learning_rate = 0.5\n",
    "epochs = 5\n",
    "weight_decay = 0.\n",
    "dropout = 0.5\n",
    "hidden_dim_lstm = 128\n",
    "hidden_dim_mlp = 128\n",
    "use_a = True\n",
    "\n",
    "# set model and optimizer\n",
    "# input is 101x4 array, 4 is in_channels \n",
    "# output is 1d since there are 2 classes; use sigmoid\n",
    "\n",
    "model = LSTM(4, hidden_dim_lstm, hidden_dim_mlp, use_a, dropout)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate,\n",
    "                      weight_decay=weight_decay)\n",
    "\n",
    "# send model over to device\n",
    "model = model.to(device)\n",
    "\n",
    "# train & validate model\n",
    "train_model(model)\n",
    "\n",
    "# test model\n",
    "test_loss, test_acc = eval_model(model, test_loader)\n",
    "print(f'Test loss: {test_loss:.6e}, accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe11a71-8d40-44ad-8dd2-1253801738bf",
   "metadata": {},
   "source": [
    "CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6e5d843b-ab86-42d0-8c18-9c07bd9839b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 1.018251e+01, Acc: 0.5488\n",
      "Val Loss: 6.237770e-01, Val Acc: 0.5664\n",
      "Epoch: 2, Loss: 6.069835e-01, Acc: 0.6361\n",
      "Val Loss: 6.081846e-01, Val Acc: 0.6150\n",
      "Epoch: 3, Loss: 5.871251e-01, Acc: 0.6601\n",
      "Val Loss: 5.916642e-01, Val Acc: 0.7172\n",
      "Epoch: 4, Loss: 5.608146e-01, Acc: 0.6821\n",
      "Val Loss: 5.552275e-01, Val Acc: 0.7207\n",
      "Epoch: 5, Loss: 5.480938e-01, Acc: 0.6887\n",
      "Val Loss: 5.456647e-01, Val Acc: 0.7252\n",
      "Test loss: 5.331877e-01, accuracy: 0.7286\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "learning_rate = 0.5\n",
    "epochs = 5\n",
    "weight_decay = 0.\n",
    "dropout = 0.5\n",
    "kernel_size = 5\n",
    "hid_channels = 32\n",
    "hidden_dim_mlp = 128\n",
    "use_a = True\n",
    "use_maxpool = True\n",
    "\n",
    "model = CNN(101, 4, 1, hid_channels, kernel_size, hidden_dim_mlp, use_a, dropout, use_maxpool)\n",
    "    \n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate,\n",
    "                      weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "# send model over to device\n",
    "model = model.to(device)\n",
    "\n",
    "# train & validate model\n",
    "train_model(model)\n",
    "\n",
    "# test model\n",
    "test_loss, test_acc = eval_model(model, test_loader)\n",
    "print(f'Test loss: {test_loss:.6e}, accuracy: {test_acc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
